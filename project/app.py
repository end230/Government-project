import os
import streamlit as st
from docx import Document

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

# Arabic-friendly multilingual embeddings (requires sentence-transformers)
# If you installed langchain-huggingface, you can switch to:
# from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings


# -----------------------------
# Settings
# -----------------------------
DOCS_FOLDER = "Data"     # put your .docx files here
PERSIST_DIR = "db"       # where Chroma stores its data
TOP_K = 10               # retrieve how many chunks
SHOW_K = 4               # show how many results
CHUNK_SIZE = 2200        # bigger chunks to include tables/lists
CHUNK_OVERLAP = 300      # overlap to avoid cutting important parts


SYSTEM_PROMPT = """
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¨Ø­Ø« Ù‚Ø§Ù†ÙˆÙ†ÙŠ.
Ø¨Ø¯ÙˆÙ† Ù†Ù…ÙˆØ°Ø¬ Ù„ØºÙˆÙŠ (LLM) Ø³ØªÙ‚ÙˆÙ… ÙÙ‚Ø· Ø¨Ù€:
- Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª.
- ØªØ¬Ù†Ø¨ ØªÙƒØ±Ø§Ø± Ù†ÙØ³ Ø§Ù„Ù…Ù‚Ø·Ø¹.
- Ù„Ùˆ Ø§Ù„Ø³Ø¤Ø§Ù„ ØªØ­ÙŠØ©/ÙƒÙ„Ø§Ù… Ø¹Ø§Ù…: Ø§Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù…ØªØ¹Ù„Ù‚Ø§Ù‹ Ø¨Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª.
- Ù„Ùˆ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬: Ø£Ø®Ø¨Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ù†Ù‡ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ Ù…Ø·Ø§Ø¨Ù‚.
"""


def is_smalltalk(q: str) -> bool:
    q = q.strip().lower()
    small = [
        "hi", "hello", "how are you", "hey",
        "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ø§Ø²ÙŠÙƒ", "Ø§Ø²Ù‰Ùƒ", "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ", "Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù‡Ù„Ø§"
    ]
    return any(s in q for s in small)


def read_docx(path: str) -> str:
    doc = Document(path)
    # collect non-empty paragraphs
    lines = [p.text.strip() for p in doc.paragraphs if p.text and p.text.strip()]
    return "\n".join(lines)


def build_vectorstore(folder=DOCS_FOLDER, persist_dir=PERSIST_DIR):
    # 1) read all docx
    texts = []
    metadatas = []

    if not os.path.isdir(folder):
        raise FileNotFoundError(f"Folder not found: {folder}")

    for fname in os.listdir(folder):
        # only .docx, skip Word temp/lock files
        if not fname.lower().endswith(".docx"):
            continue
        if fname.startswith("~$"):
            continue

        full_path = os.path.join(folder, fname)

        # if file path doesn't exist / broken shortcut
        if not os.path.isfile(full_path):
            continue

        content = read_docx(full_path)
        if content.strip():
            texts.append(content)
            metadatas.append({"source": fname})

    if not texts:
        raise ValueError(f"No .docx files found (or all empty) in folder: {folder}")

    # 2) split into chunks (bigger chunks are better for legal tables/lists)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""],
    )

    chunks = []
    chunk_meta = []

    # We'll also store the raw chunks per source to support "before/after" context
    raw_chunks_by_source = {}

    for text, meta in zip(texts, metadatas):
        parts = splitter.split_text(text)

        # keep for "context around"
        raw_chunks_by_source[meta["source"]] = parts

        for j, p in enumerate(parts):
            chunks.append(p)
            chunk_meta.append({**meta, "chunk_id": j})

    # 3) embeddings
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )

    # 4) vector store (Chroma)
    vectordb = Chroma.from_texts(
        texts=chunks,
        embedding=embeddings,
        metadatas=chunk_meta,
        persist_directory=persist_dir,
    )

    # Chroma persists automatically in newer versions; no need for vectordb.persist()

    return vectordb, raw_chunks_by_source


# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="Arabic Word RAG (No LLM)", layout="wide")
st.title("ğŸ“š Arabic Word RAG Chatbot (Ø¨Ø¯ÙˆÙ† LLM Ø­Ø§Ù„ÙŠØ§Ù‹)")
st.caption("âš™ï¸ Ù‡Ø°Ø§ Ø§Ù„Ø¥ØµØ¯Ø§Ø± ÙŠØ¹Ø±Ø¶ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© Ù…Ù† Ù…Ù„ÙØ§Øª Word. Ù„Ø§Ø­Ù‚Ø§Ù‹ Ø³Ù†Ø¶ÙŠÙ LLM Ù„ØªÙ„Ø®ÙŠØµ ÙˆØ¥Ø¹Ø·Ø§Ø¡ Ø¥Ø¬Ø§Ø¨Ø© Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø©.")

# Build / Load DB once
if "vectordb" not in st.session_state:
    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨Ø­Ø« (Ø£ÙˆÙ„ Ù…Ø±Ø© Ù‚Ø¯ ØªØ£Ø®Ø° ÙˆÙ‚ØªØ§Ù‹)..."):
        vectordb, raw_chunks_by_source = build_vectorstore()
        st.session_state.vectordb = vectordb
        st.session_state.raw_chunks_by_source = raw_chunks_by_source

if "history" not in st.session_state:
    st.session_state.history = []


# Show system prompt for clarity
with st.expander("ğŸ§  ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… (System Prompt)"):
    st.write(SYSTEM_PROMPT.strip())

question = st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")

if question:
    st.session_state.history.append(("user", question))

    # handle small talk
    if is_smalltalk(question):
        answer = "Ù…Ø±Ø­Ø¨Ø§Ù‹ ğŸ‘‹ Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù…ØªØ¹Ù„Ù‚Ø§Ù‹ Ø¨Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª (Ù…Ø«Ù„Ø§Ù‹: Ù…Ø§ Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø© 36ØŸ)."
        st.session_state.history.append(("assistant", answer))
    else:
        retriever = st.session_state.vectordb.as_retriever(search_kwargs={"k": TOP_K})
        docs = retriever.invoke(question)

        # deduplicate by normalized text
        seen = set()
        unique_docs = []
        for d in docs:
            txt_norm = " ".join(d.page_content.split())
            if txt_norm in seen:
                continue
            seen.add(txt_norm)
            unique_docs.append(d)
        docs = unique_docs

        if not docs:
            answer = "Ù„Ù… Ø£Ø¬Ø¯ Ù†ØµØ§Ù‹ Ù…Ø·Ø§Ø¨Ù‚Ø§Ù‹ Ø£Ùˆ Ù‚Ø±ÙŠØ¨Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª."
            st.session_state.history.append(("assistant", answer))
        else:
            # Build an organized answer (no LLM: we show relevant parts)
            answer = "âœ… **Ø£Ù‚Ø±Ø¨ Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:**\n\n"
            for i, d in enumerate(docs[:SHOW_K], start=1):
                src = d.metadata.get("source", "unknown")
                cid = d.metadata.get("chunk_id", None)
                answer += f"### {i}) Ø§Ù„Ù…ØµØ¯Ø±: {src}\n"
                answer += d.page_content.strip() + "\n\n"
                if cid is not None:
                    answer += f"*(chunk_id: {cid})*\n\n"

            st.session_state.history.append(("assistant", answer))


# Render chat history
for role, msg in st.session_state.history:
    with st.chat_message(role):
        st.write(msg)

# Optional: show full context (before/after) for the top retrieved doc
if st.session_state.history:
    # Find last assistant message and if it had docs, allow user to explore context
    st.divider()
    st.subheader("ğŸ” Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ + Ø³ÙŠØ§Ù‚ Ù‚Ø¨Ù„/Ø¨Ø¹Ø¯ (Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©)")

    # We'll rerun retrieval only if the last user message exists
    last_user = None
    for r, m in reversed(st.session_state.history):
        if r == "user":
            last_user = m
            break

    if last_user and not is_smalltalk(last_user):
        retriever = st.session_state.vectordb.as_retriever(search_kwargs={"k": TOP_K})
        docs = retriever.invoke(last_user)

        # dedup again
        seen = set()
        unique_docs = []
        for d in docs:
            txt_norm = " ".join(d.page_content.split())
            if txt_norm in seen:
                continue
            seen.add(txt_norm)
            unique_docs.append(d)
        docs = unique_docs

        if docs:
            top = docs[0]
            src = top.metadata.get("source", "unknown")
            cid = top.metadata.get("chunk_id", None)

            st.write(f"**Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© Ù…Ù†:** `{src}`")

            with st.expander("ğŸ“„ Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹"):
                st.write(top.page_content)

            # before/after context (if available)
            all_chunks = st.session_state.get("raw_chunks_by_source", {}).get(src, [])
            if cid is not None and all_chunks:
                before = all_chunks[cid - 1] if cid - 1 >= 0 else ""
                after = all_chunks[cid + 1] if cid + 1 < len(all_chunks) else ""

                with st.expander("ğŸ§© Ø¹Ø±Ø¶ Ø§Ù„Ø³ÙŠØ§Ù‚ (Ù‚Ø¨Ù„/Ø¨Ø¹Ø¯) Ù„Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ù‚ÙˆØ§Ø¦Ù…"):
                    if before:
                        st.markdown("**Ù‚Ø¨Ù„:**")
                        st.write(before)
                    st.markdown("**Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:**")
                    st.write(top.page_content)
                    if after:
                        st.markdown("**Ø¨Ø¹Ø¯:**")
                        st.write(after)
        else:
            st.write("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø³ÙŠØ§Ù‚.")
    else:
        st.write("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù…ØªØ¹Ù„Ù‚Ø§Ù‹ Ø¨Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø³ÙŠØ§Ù‚.")
